# Machine Learning

## Preliminary understanding of machine learning

### 1.What is machine learning

- Definition: 
  - Field of study that gives computers the ability to learn without being explicitly programmed.

### 2. Supervised learning

- Definition:
  - "right answers" given
- Regression:
  - Perdict continuous valuded output.
- Classification:
  - Discrete valued ouput

- Both regression and classification are called predict problem.

### 3. Unsupervised learning

- Clustering:
  - no "right answers" given
  - computer are expected to sort the samples by itself.
- Separated overlapping audio by itself.

## Linear regression with one variable

### 1.Model representation

- Notation:

  - m = Number of training examples

  - x's = "input" variable/features

  - y's = "output" varibale/features

  - (x,y) = one training example

  - $(x^{(i)},y^{(i)})$ = $i^{th}$ training example

- Flow chart:
  
- ![image-20201204200548126](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204200548126.png)
  
- How do we represent h?

  - ![image-20201204200750830](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204200750830.png)

  - $$
     h(x) = \theta_0+\theta_1x
    $$

### 2.Cost function

#### Intruction

- Idea:

  - Choose $\theta_0$ and $\theta_1$ so that h(x) is close to y for our training examples (x,y)

- Cost function:
  
  
  - $$
    minimize\space J(\theta_0,\theta_1) = \frac1{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2
	$$

#### Unit 1

- The value of J with diffrent $\theta_1$
  - ![image-20201204204733459](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204204733459.png)

#### Unit 2

- The value of J with diffrent $\theta_1$ and $\theta_0$
  - ![image-20201204205745219](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204205745219.png)

### 3.Gradient descent

- Outline

  - Start with some $\theta_1$ and $\theta_0$
  - Keep changing $\theta_1$ and $\theta_0$ to reduce J until we hopefully end up at a minimum.

- Feature of gradient descent:

  - you will get different local optimum with different starting points.
  - ![image-20201204210904727](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204210904727.png)

  

- Gradient descent algorithm:

  - $$
    \theta_j:=\theta_j-\alpha\frac\partial{\partial\theta_j}J(\theta_0,\theta_1)
    $$

  - $\alpha$: Learning rate

  - Simultaneous update:

    - ![image-20201204211859365](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204211859365.png)

    

- The value of the derivative represents the opposite direction of lowering the cost function.
- $\alpha$:too small or too large
  
- ![image-20201204213304203](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204213304203.png)
  
- As gradient descent runs,the sept will get smaller and smaller.

- Local minimun is when you have this derivative equal to zero.

  

- ![image-20201204215841240](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204215841240.png)
  - The x in the last row in the figure above is derived by taking the derivative.

  

- ![image-20201204220629092](C:\Users\lenovo\Desktop\machine_learning\images\image-20201204220629092.png)

  - The cost function of  linear regression is a convex fuction.It's doesn't have any local optima,except for the one global optimum.

- "Batch" Gradient Descent:

  - "Batch":Each step of gradient descent uses all the training examples.
  

## Linear Regression with multiple variables

### 1.Multiple features
- Notation:

  - n = Number of training examples
- $x^{(i)}$ = input(features) of $i^{th}$ training example
  
- $x_j^{(i)}$ = value of features j in  $i^{th}$ training example
  
- Hypothesis:

  - $$
    h(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...\theta_nx_n = \theta^TX
    $$

  - define $x_0$ = 1

### 2.Gradient descent for multiple variables

- Cost function:

  - $$
    J(\theta_0,\theta_1,...\theta_n)= \frac1{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2
    $$

- Gradient descent algorithm:

  - $$
    \theta_j:=\theta_j-\alpha\frac\partial{\partial\theta_j}J(\theta_0,\theta_1,...\theta_n)
    $$

  - ![image-20201205135233681](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205135233681.png)

### 3.Gradient descent in practice 1:Feature Scaling

- If The range of two feature values differ greatly, it may take a long time for the model to converge.
- Feature Scaling:
  
- ![image-20201205140145951](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205140145951.png)
  
- Mean normalization:

  - ![image-20201205140718667](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205140718667.png)

  - $$
    x_i = \frac{x_i-u_i}{s_i}
    $$

    - where $s_i$ is the range of $x_i$ (max - min)

### 3.Gradient descent in practice 2:Learning Rate

- Example automatic convergence test:
  - Declare convergence if J decreases by less than $10^{-3}$ in one iteration.
- If J is increasing, try to use a smaller learning rate.
- ![image-20201205142716392](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205142716392.png)

- Summary:
  - If $\alpha$ is too small:slow concergence.
  - If $\alpha$ is too large:J may not decrease on every iteration;may not converge.
  - ![image-20201205143129108](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205143129108.png)



### 4.Features and polynomial regression

- ![image-20201205143840997](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205143840997.png)

- When you use the square model, y goes down as you increase x(x is large enough).You can consider the cubic model.

- It's important to apply feature scaling when you use polynomial regression.
- Rather than going to a cubic model,you can choose square root function.
  - ![image-20201205144657691](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205144657691.png)



### 5.Normal equation

- Normal equation: Method to solver for $\theta$ analytically.

- $$
  J(\theta_0,\theta_1,...\theta_n)= \frac1{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2
  $$

  - Take the partial derivative with respect to $\theta$ to minimize J

- ![image-20201205145850452](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205145850452.png)

- use:
  $$
  \theta = (X^TX)^{-1}X^Ty
  $$
  to get the $\theta$ to minimize J.

- The normal equation no need normalization.

- Advantaes and disadvantages:
- Normal equation and non-invertibility
  
  - ![image-20201205151826769](C:\Users\lenovo\Desktop\machine_learning\images\image-20201205151826769.png)

## Logistic Regression

### 1.Classification

- y = 0 or y =1 ,but linear regression h(x) can be >1 or<0
- Logistic Regression:0<=h(x)<=1
- Logistic regression is used for binary classification.

### 2.Hypothesis Representation

- $$
  h(x) = \frac1{1+e^{-\theta^Tx}}
  $$

- Sigmoid function

  - $$
    g(z) = \frac1{1+e^{-z}}
    $$

  - ![image-20201208111655670](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208111655670.png)

- h(x) = estimated probability that y = 1 on input x.
- h(x) = P(y=1|x;$\theta$)

### 3.Decision boundary

- Decision boundary

  - ![image-20201208113144226](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208113144226.png)

  - x1 + x2 = 3

- Non-linear decision boundaries

  - ![image-20201208113855543](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208113855543.png)

### 4.Cost function

- If using the cost function of linear regression,you will get a non-convex loss function because the sigmoid funcion is non linearity.The non-convex function have many local optimium.
  
- ![image-20201208114551577](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208114551577.png)
  
- Cost function:

  - $$
    Cost(h(x),y) = 
    \begin{cases}
    -log(h(x))\space if\space y=1\\
    -log(1-h(x))\space if\space y=0\\
    \end{cases}
    $$

  - ![image-20201208115659141](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208115659141.png)

### 5.Simplified cost function and gradient descent

- Simplified cost function:

  - $$
    Cost(h(x),y) = -ylog(h(x))-(1-y)log(1-h(x))
    $$

  - $$
    J(\theta_0,\theta_1,...\theta_n)= -\frac1{m}[\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)}))]
    $$

    

- Gradient Descent:

  - $$
    \theta_j = \theta_j -\alpha\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)}
    $$

  - same with linear regression

### 6.Advanced optimization

- ![image-20201208123718402](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208123718402.png)

- No need to konw how it work,becase it's vary complex.

### 7.Multi-class classification One-vs all

- Train a logistic regression classifier $h^{(i)}(x)$ for each class i to predict the probability that y = i.
- One new input x,to make a prediction,pick the class i that maximizes max $h^{(i)}(x)$ 

## Regularization

### 1.The problem of overfitting

- If we have a lot of features and very little training data,then overfitting can become a problem.
- Adressing overfitting:
  - Reduce number of features
    - Manually select
    - Model selection algorithm.
  - Regularization
    - Reduce magnitude/values of parameters $\theta$

### 2.Cost function

- ![image-20201208155148672](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208155148672.png)

- Regularization

  - Small values for parameters

    - "Simpler" hypothesis

    - Less prone to overfitting

  - If we don't know which one are less likely to be relevant

    - $$
      J(\theta_0,\theta_1,...\theta_n)= \frac1{2m}[\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 + \lambda\sum_{i=1}^{n}\theta_j^2]
      $$

    - $\lambda\sum_{i=1}^{n}\theta_j^2$ is a regularization term.

    - The $\lambda$ is to control the tarde of between these two goals:

      - Fitting the training set well.
      - Keeping the parameter small and therefore keeping the hypothesis relatively simple to avoid overfitting.

    - ![image-20201208160515953](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208160515953.png)

    - If $\lambda$ is too large,then h(x) = $\theta_0$

### 3.Regularization linear regression

- Gradient descent:

  - $$
    \theta_0 = \theta_0 -\alpha\frac1m\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_0^{(i)}
    $$

  - $$
    \theta_j = \theta_j -\alpha[\frac1m\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}m\theta_j]
    $$

  - $$
    \theta_j = \theta_j(1-\alpha \frac{\lambda}m) -\alpha\frac1m\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)}
    $$

    - The first terms is little than $\theta_j$,such as 0.99$\theta_j$,

- Normal equation

  - $$
    \theta = (X^TX+\begin{bmatrix}
    0\\
    &1\\
    &&1\\
    &&&\ddots\\
    &&&&1
    \end{bmatrix})^{-1}X^Ty
    $$

### 4.Regularized logistic regression

- Cost function:

  - $$
    J(\theta_0,\theta_1,...\theta_n)= -[\frac1{m}\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)}))] + \frac{\lambda}{2m}\sum_{i=1}^{m}\theta_j^2
    $$

- Gradient descent:

    - $$
      \theta_0 = \theta_0 -\alpha\frac1m\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_0^{(i)}
      $$

    - $$
      \theta_j = \theta_j -\alpha[\frac1m\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}m\theta_j]
      $$


## Neural Networks:Representation

### 1.Non-linear hypotheses

- ![image-20201208165635116](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208165635116.png)

- Transform multiple eigenvalues into two sets of eigenvalues

### 2.Model representataion 1

-  ![image-20201208171412640](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208171412640.png)

- It contains the input layer, the hidden layer and the output layer.
- ![image-20201208171918266](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208171918266.png)

### Model representataion 2

- Forward propagation:Vectorized implementation:
	- x = $\begin{bmatrix}
  x_0\\
  x_1\\
  x_2\\
  x_3
  \end{bmatrix}$     $z^{(2)}$ = $\begin{bmatrix}
  z_1^{(2)}\\
  z_2^{(2)}\\
  z_3^{(2)}\\
  \end{bmatrix}$
- $z^{(2)}=\theta^{(1)}x$,   $a^{(2)}=g(z^{(2)})$, define $a^{(1)}$ = x,$z^{(2)}=\theta^{(1)}a^{(1)}$
	- Add $a^{(0)}$ = 1
- $z^{(3)}=\theta^{(2)}a^{(2)}$
	- h(x) = $a^{(2)}$ = g($z^{(3)}$)
	- ![image-20201208174118175](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208174118175.png)
- Neural Network is like logistic regression with new features a1,a2,a3

### Examples and intuitions 1

- Simple example: AND
  - ![image-20201208175752113](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208175752113.png)

- simple example: OR
  - ![image-20201208175905198](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208175905198.png)

### Examples and intuitions 2

- x1 XNOR x2
  - ![image-20201208180442298](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208180442298.png)

### Multi-class classification

- ![image-20201208181549857](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208181549857.png)

## Neural Networks:Learning

### 1.Cost function

- Binary classification and Multi-class classification(K classes):
  
- ![image-20201208194455546](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208194455546.png)
  
- Cost function

  - $$
    J= -[\frac1{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h(x^{(i)}))_k+(1-y_k^{(i)})log(1-h(x^{(i)}))_k] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_1}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^l)^2
    $$

  - h(x) $R^K$ ,$(h(x))_i = i^{th}$

  - K is the number of outputs.

  - L is the number of layers in network.

  - $s_l$ is the number of units(not conunting bias unit) in layer l.

### 2.Backpropagation algorithm

-  ![image-20201208203554974](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208203554974.png)

- ![image-20201208204218870](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208204218870.png)

### Backpropagation intuition

- $\delta$ turn out to be the partial dervative of the cost function with respect to these intermediate terms and they are measure of how much would we like to change the neural network's weights in order to affect these intermediate values of the computation,so as to affect the final ouput that the neural network h of x and therefore affect the overall cost
- ![image-20201208210358045](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208210358045.png)

- ![image-20201208210938602](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208210938602.png)

### Gradient checking

- ![image-20201208213516460](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208213516460.png)

### Random initialization

- ![image-20201208214024964](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208214024964.png)

### Putting it together

- ![image-20201208214659901](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208214659901.png)
- ![image-20201208215019859](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208215019859.png)

- ![image-20201208215244660](C:\Users\lenovo\Desktop\machine_learning\images\image-20201208215244660.png)


## Advce for applying machine learning

### Deciding what to try next.

- Improve model
  - Get more training examples.
  - Try smaller sets of features.
  - Try getting additional features.
  - Try adding polynomial features.($x_1^2,x_2^2,x_1x_2,etc.$)
  - Try decreasing or increasing $\lambda$

### Evaluating a hypothesis

- Shuffle and split datasets to training sets and test sets.(7:3)
- Calculate the loss on the training set and test set.
- Calculate misclassification error.

### Model selection and training/validation/test sets

- ![image-20201210140504888](C:\Users\lenovo\Desktop\machine_learning\images\image-20201210140504888.png)
- Using training data to choose parameters $\theta$ by J.
- Using test data to choose models(d) by J.But the model may only fit to test set.
- Split datasets to training sets,validation sets and test sets.(6:2:2)
- Using validation to choose models(d) by J and using test sets to estimate generalization error.

### Diagnosing bias vs. variance

- ![image-20201216113830582](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216113830582.png)

- High bias: undefit
  - J_train will be high
  - J_test  = J_train
- Hign variance: overfit
  - J_trian will be low
  - J_test >> J_train

### Regularization and bias/variance

- ![image-20201216114729838](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216114729838.png)

- ![image-20201216115611112](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216115611112.png)

  - use J to train model
  - use J_train to evaluate generalization ability on the training set.
  - use J_cv to evaluate generalization ability on the validation set.

  - use J_test to evaluate generalization ability on the test set.

- ![image-20201216115952113](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216115952113.png)
  - Try different $\lambda$ and pick one by J_cv,then use J_test to evaluate generalization ability.

- ![image-20201216120403132](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216120403132.png)

### Learning curves

- ![image-20201216121028444](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216121028444.png)

- High bias

  - ![image-20201216121505766](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216121505766.png)

  - If a learning algorithm has high bias,as we get more and more training examples,the crossvalidation error basically fattened up.If the algorith is really suffering from high bias, get more training data  will actually not (by itself) help that much.

- High variance
  - ![image-20201216122212567](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216122212567.png)
  - If the algorith is really suffering from high variance, get more training data  is likely to help.

### Deciding what to try next (revisited)

- ![image-20201216122911938](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216122911938.png)
- ![image-20201216123225893](C:\Users\lenovo\Desktop\machine_learning\images\image-20201216123225893.png)



## Machine learning system design

### Error analysis

- Create a simple model and analysis the examples misclassified to improve the model.
- Analysis the learning curves and choose some methods to imporve it.

### Error metrics for skewed classes

- ![image-20201217155940987](C:\Users\lenovo\Desktop\machine_learning\images\image-20201217155940987.png)
- If a algorithm has high precision an recall,we can say it's really a good algorithm even if we have very skewed clases.

### Trading off precision and recall

- ![image-20201217161622240](C:\Users\lenovo\Desktop\machine_learning\images\image-20201217161622240.png)
- Change the threshould to get high precision or recall.
- ![image-20201217162240205](C:\Users\lenovo\Desktop\machine_learning\images\image-20201217162240205.png)

### Data for machine learning

- ![image-20201218112916350](C:\Users\lenovo\Desktop\machine_learning\images\image-20201218112916350.png)
- Use many parameters to get low bias and use large training set to get low variance.

## Support Vector Machines

### Optimizaton objective

- ![image-20201218114235220](C:\Users\lenovo\Desktop\machine_learning\images\image-20201218114235220.png)

- Logistic regression:
	- $$
  J(\theta_0,\theta_1,...\theta_n)= \frac1{m}\sum_{i=1}^{m}y^{(i)}(-log(h(x^{(i)})))+(1-y^{(i)})(-log(1-h(x^{(i)}))) + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
  	$$

- SVM:

  - $$
    J(\theta_0,\theta_1,...\theta_n)= C\sum_{i=1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)}) + \sum_{j=1}^{n}\theta_j^2
    $$

- SVM use "C" to trade off the first item and the second item instead of $\lambda$  

- ![image-20201218115909124](C:\Users\lenovo\Desktop\machine_learning\images\image-20201218115909124.png)

- SVM will output 1 or 0 instead the probility.

### Large Marigin Intuition

- ![image-20201218120453857](C:\Users\lenovo\Desktop\machine_learning\images\image-20201218120453857.png)
- ![image-20201218121037356](C:\Users\lenovo\Desktop\machine_learning\images\image-20201218121037356.png)
- ![image-20201218121456672](C:\Users\lenovo\Desktop\machine_learning\images\image-20201218121456672.png)
- If you have a large "C",the line will change from black one to pink one.

### The mathematics behind large margin classification

- ![image-20201219160143728](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219160143728.png)
- Make p as large as you can to make $p||\theta||$>=1(<=-1).so you can get the large margin.
- Actuall you can make "C" enough large than you will get large 'p'.But the classifier will be very sensitive to nosie.(The second picture above)
- The green line,decision boundary: $\theta^T x=0$

### Kernels 1

- Similarity kernel:
- ![image-20201219164059781](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219164059781.png)
- ![image-20201219164231764](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219164231764.png)
- Given a x, we can get 3 features:f1,f2,f3
- ![image-20201219164611405](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219164611405.png)
- ![image-20201219165317712](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219165317712.png)
- Given 3 points(l), if x in the area near some of the points,it will be predicted as 1, others 0

### Kernels 2

- Where to get points l:

  - Use x as l. If we have m samples,we will get m points l
  - ![image-20201219170229528](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219170229528.png)
  - x -> f

- ![image-20201219171114332](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219171114332.png)

  - $$
    J(\theta_0,\theta_1,...\theta_n)= C\sum_{i=1}^{m}y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)}) + \sum_{j=1}^{n}\theta_j^2
    $$

- ![image-20201219172206653](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219172206653.png)

  - Large $\sigma^2$ : unfit,Small $\sigma$: overfit

### Use an SVM

- ![image-20201219172852691](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219172852691.png)
- ![image-20201219173528257](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219173528257.png)
- If not feature scaling,the 'f' may be decided by one feature of 'x'.For example,if (x1-l1) is vary large,then ||x-l|| is nearly equal to (x1-l1)
- ![image-20201219174231628](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219174231628.png)
- Multi-class classification:
  - ![image-20201219181420052](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219181420052.png)

- Logistic regression vs. SVMs

  - ![image-20201219182541722](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219182541722.png)
  - For the most time,Logistic regression and SVM without kernel are very similar.
## Clustering

### K-means algorithm

- ![image-20201219193636134](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219193636134.png)
- K-means
  - 1.initialize K points randomly
  - 2.Calculate the distance between x and k points,classify x as the kth class which is the nearest point to x.
  - 3. Calculate the average point of each class.
  - 4. Make the average points as k potiots
  - 5. Repeat step 2,3,4 until k points don't change.

### Optimization objective

- ![image-20201219200016142](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219200016142.png)

- $$
  J(c^{(1)},...,c^{(m)},\mu_1,...,\mu_K) = \frac1m\sum^m_{i=1}||x^{(i)}-\mu_{c^{(i)}}||^2
  $$

- Get $c^{(i)}\space and\space\mu_k$ to min J

- Step 2 minimizes J by changing c

- Step 3,4 minimizes J by changing $\mu$

### Random initizalization

- ![image-20201219201121345](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219201121345.png)

  

- ![image-20201219201320310](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219201320310.png)

- The bottom two graphs show that we get a local minimum of J

- If we want the top graph,we can try multiple random initializations.
- ![image-20201219201838577](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219201838577.png)
- It only works if K is small.

### Choosing the number of clusters

- ![image-20201219202528802](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219202528802.png)

## Dimensionality Reduction

### Data Compression

- ![image-20201219220200996](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219220200996.png)
- ![image-20201219220451785](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219220451785.png)

### Principal Component Analysis problem formulation(PCA)

- ![image-20201219221959492](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219221959492.png)
- PCA is not linear regression.Both are minimize the distance,but the the distance are defined differently.
- ![image-20201219222234026](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219222234026.png)

- PCA:It's trying to find a lower dimensional surface onto which to project the data so as to minimize the squared projection error.

### Principal Component Analysis algorithm

- ![image-20201219222955481](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219222955481.png)

- ![image-20201219223818150](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219223818150.png)

- Gets the first k columns as U_reduce

- $$
  z^{(i)} = U_{reduce}^Tx^{(i)}
  $$

- not done x_0  = 1

- ![image-20201219224530430](C:\Users\lenovo\Desktop\machine_learning\images\image-20201219224530430.png)



### Reconstruction from compressed representation

- ![image-20201221122754259](C:\Users\lenovo\Desktop\machine_learning\images\image-20201221122754259.png)

### Choosing the number of principal components

- ![img](https://img-blog.csdn.net/20180718153749949?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dlbml1czlfOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- As the dimension increases, the proportion of error decreases.

- The proportion of error can be calculated by

  - $$
    \frac{\frac1m\sum^m_{i=1}||x^{(i)}-x^{(i)}_{approx}||^2}{\frac1m\sum^m_{i=1}||x^{(i)}||^2}<=0.01
    $$

- The above formula shows that 99% of the information can be retained after dimension reduction.

### Advice for applying PCA

- Application of PAC:
  - data compression
  - data visualization
  - speed up th algorithm

## Anomaly detection

### Problem motivation

- ![image-20201221180019179](C:\Users\lenovo\Desktop\machine_learning\images\image-20201221180019179.png)

- Use dataset(considered normal) to train a probability model 'p'
- Use the model to get the probability of a new data.
- Compare probability and thresholdsto see if it is an exception data.

### Algorithm

- ![image-20201221181318532](C:\Users\lenovo\Desktop\machine_learning\images\image-20201221181318532.png)

### Anomaly detection vs. supervised learning

- ![image-20201222152918689](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222152918689.png)
- Decided by the number of anomaly examples.

### Choosing what features to use

- ![image-20201222155923297](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222155923297.png)

### Multivariate Gaussian distribution

- ![image-20201222161618255](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222161618255.png)
- p(x1) and p(x2) may be both not low, but p(x1)*p(x2) may be low.
- ![image-20201222162034229](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222162034229.png)
- ![image-20201222162054697](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222162054697.png)
- ![image-20201222162212727](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222162212727.png)
- ![](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222162340204.png)
- Multivariate Gaussian distributions allow you to describe situations where two features are positively or negatively correlated.

### Anomaly detection using the multivariate Gaussian distribution

- ![image-20201222162818090](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222162818090.png)
- ![image-20201222163019834](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222163019834.png)
- ![image-20201222163331417](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222163331417.png)
- ![image-20201222163705082](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222163705082.png)

## Recommender Systems

### Problem formulation

- ![image-20201222165136215](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222165136215.png)
- Try to pridict the '?' and recommend new movies to users.

### Conten-based recommendations

- ![image-20201222165841206](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222165841206.png)
- Make each movie be a 'x' with 3 features.
- Each use has a $\theta^{(i)}$
- Use $\theta^{(i)}x^{(j)}$ to predict user i's rating of movie j.
- ![image-20201222170532249](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222170532249.png)
- ![image-20201222170805924](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222170805924.png)
- It's like linear regression but it doesn't have '1/m'

### Collaborative filtering

- ![image-20201222171646068](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222171646068.png)
- First get $\theta^{(i)}$ and use it to get feature values of movies.
- ![image-20201222172243525](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222172243525.png)
- ![image-20201222172732695](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222172732695.png)
- initialize $\theta$ randomly and use it to get x, use x to update $\theta$,repeat that.(base on that each user rates moveis and each movie is rated by many each users.)

### Collaborative filtering algorithm

- ![image-20201222173837256](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222173837256.png)
- ![image-20201222174150639](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222174150639.png)

### Vectorization: Low rank matrix factorization

- ![image-20201222174805979](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222174805979.png)
- ![image-20201222175246631](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222175246631.png)

### Implementational detail: Mean normalization

- ![image-20201222180146367](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222180146367.png)
- Recommend moveis to new users who don't rate any movies.

## large scale machine learning

### Stochastic gradient descent

- ![image-20201222215327230](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222215327230.png)
- ![image-20201222215811791](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222215811791.png)
- Stochastic gradient descent doesn't need all examples to get the derivation of J.
- It only needs one sample at a time to make the algorithm fit that sample a little better.

### Mini-batch gradient descent

- ![image-20201222220610913](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222220610913.png)
- ![image-20201222220631331](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222220631331.png)

### Stochastic gradient descent convergence

- ![image-20201222221454642](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222221454642.png)
- ![image-20201222221925696](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222221925696.png)
- ![image-20201222222047222](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222222047222.png)

### Online learning

- ![image-20201222222829760](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222222829760.png)
- ![image-20201222223400806](C:\Users\lenovo\Desktop\machine_learning\images\image-20201222223400806.png)

### Map-reduce and data parallelism

- ![image-20201223163457955](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223163457955.png)
- ![image-20201223163609646](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223163609646.png)

## Application examle Photo OCR

### Problem description and pipline

- ![image-20201223164752001](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223164752001.png)

### Sliding windows

- 1).Train a classifier to determine whether a picture contains pedestrains with fixed length-width ratio.
	- ![image-20201223165910019](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223165910019.png)

- 2).Make a rectangular with a specific aspect ratio slide on the image.
  - ![image-20201223171034435](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223171034435.png)
- 3).Determine which picture contains a pedestrain by classifer.
- Using rectangular with a different aspect ratio to repeat step 2).(The images with different aspect ratio can be re-sized to a image with specific aspect ratio.)
- ![image-20201223171058791](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223171058791.png)
- ![image-20201223172200639](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223172200639.png)
- Determine if there are any white pixels around the white pixels, and if so, connect them together.
- Delete those pixel boxes that are not normal in aspect radio.
- ![image-20201223172613889](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223172613889.png)
- ![image-20201223173035269](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223173035269.png)

### Getting lots of data: Artificial data synthesis

- ![image-20201223175011012](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223175011012.png)

### Ceiling analysis: What part of the pipline to work on next

- ![image-20201223175849602](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223175849602.png)

### Summary
- ![image-20201223181931428](C:\Users\lenovo\Desktop\machine_learning\images\image-20201223181931428.png)

  

  